{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6337c0b",
   "metadata": {},
   "source": [
    "# Custom Knowledge Chatbot w/ LlamaIndex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8911e71",
   "metadata": {},
   "source": [
    "Examples:\n",
    "- https://gita.kishans.in/\n",
    "- https://www.chatpdf.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c425f155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama_index\n",
      "  Downloading llama_index-0.5.12.tar.gz (174 kB)\n",
      "     ------------------------------------- 174.8/174.8 kB 10.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting dataclasses_json\n",
      "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
      "Collecting langchain>=0.0.123\n",
      "  Downloading langchain-0.0.137-py3-none-any.whl (518 kB)\n",
      "     ------------------------------------- 518.3/518.3 kB 16.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from llama_index) (1.22.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from llama_index) (8.2.2)\n",
      "Requirement already satisfied: openai>=0.26.4 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from llama_index) (0.27.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from llama_index) (1.5.3)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.3.3-cp39-cp39-win_amd64.whl (579 kB)\n",
      "     ------------------------------------- 579.8/579.8 kB 18.4 MB/s eta 0:00:00\n",
      "Collecting pydantic<2,>=1\n",
      "  Downloading pydantic-1.10.7-cp39-cp39-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 46.2 MB/s eta 0:00:00\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.8.4-cp39-cp39-win_amd64.whl (323 kB)\n",
      "     ------------------------------------- 323.6/323.6 kB 19.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from langchain>=0.0.123->llama_index) (2.28.2)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2\n",
      "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "     ---------------------------------------- 90.0/90.0 kB 5.3 MB/s eta 0:00:00\n",
      "Collecting SQLAlchemy<2,>=1\n",
      "  Downloading SQLAlchemy-1.4.47-cp39-cp39-win_amd64.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 33.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from langchain>=0.0.123->llama_index) (6.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from langchain>=0.0.123->llama_index) (4.0.2)\n",
      "Collecting marshmallow-enum<2.0.0,>=1.5.1\n",
      "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting typing-inspect>=0.4.0\n",
      "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.3.0\n",
      "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "     ---------------------------------------- 49.1/49.1 kB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from openai>=0.26.4->llama_index) (4.65.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from pandas->llama_index) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from pandas->llama_index) (2.8.2)\n",
      "Collecting regex>=2022.1.18\n",
      "  Downloading regex-2023.3.23-cp39-cp39-win_amd64.whl (267 kB)\n",
      "     ---------------------------------------- 268.0/268.0 kB ? eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->llama_index) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->llama_index) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->llama_index) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->llama_index) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->llama_index) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->llama_index) (1.7.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses_json->llama_index) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from pydantic<2,>=1->langchain>=0.0.123->llama_index) (4.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->llama_index) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from requests<3,>=2->langchain>=0.0.123->llama_index) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from requests<3,>=2->langchain>=0.0.123->llama_index) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from requests<3,>=2->langchain>=0.0.123->llama_index) (2022.12.7)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-2.0.2-cp39-cp39-win_amd64.whl (192 kB)\n",
      "     ---------------------------------------- 192.1/192.1 kB ? eta 0:00:00\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from tqdm->openai>=0.26.4->llama_index) (0.4.6)\n",
      "Building wheels for collected packages: llama_index\n",
      "  Building wheel for llama_index (setup.py): started\n",
      "  Building wheel for llama_index (setup.py): finished with status 'done'\n",
      "  Created wheel for llama_index: filename=llama_index-0.5.12-py3-none-any.whl size=259558 sha256=cbfb71acb2d6d5bb7d7fb86ed6db8bb4e008a8df95d0e5dbc335c7b33b04c6de\n",
      "  Stored in directory: c:\\users\\hyu\\appdata\\local\\pip\\cache\\wheels\\d8\\4b\\67\\8d91637d5d435765049fab2b08c9580372266f4228b567bb25\n",
      "Successfully built llama_index\n",
      "Installing collected packages: regex, pydantic, mypy-extensions, marshmallow, greenlet, typing-inspect, tiktoken, SQLAlchemy, openapi-schema-pydantic, marshmallow-enum, aiohttp, dataclasses_json, langchain, llama_index\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.8.1\n",
      "    Uninstalling aiohttp-3.8.1:\n",
      "      Successfully uninstalled aiohttp-3.8.1\n",
      "Successfully installed SQLAlchemy-1.4.47 aiohttp-3.8.4 dataclasses_json-0.5.7 greenlet-2.0.2 langchain-0.0.137 llama_index-0.5.12 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 pydantic-1.10.7 regex-2023.3.23 tiktoken-0.3.3 typing-inspect-0.8.0\n",
      "Requirement already satisfied: langchain in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (0.0.137)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from langchain) (1.22.3)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from langchain) (1.10.7)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from langchain) (1.4.47)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from langchain) (2.28.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.1.1)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from pydantic<2,>=1->langchain) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\hyu\\anaconda3\\envs\\openapi\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama_index\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1041eae8",
   "metadata": {},
   "source": [
    "# Basic LlamaIndex Usage Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec6395b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "openai_key='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'\n",
    "os.environ['OPENAI_API_KEY'] =openai_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cf41880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load you data into 'Documents' a custom type by LlamaIndex\n",
    "\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('./data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98df5bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 1321 tokens\n"
     ]
    }
   ],
   "source": [
    "# Create an index of your documents\n",
    "\n",
    "from llama_index import GPTSimpleVectorIndex\n",
    "\n",
    "index = GPTSimpleVectorIndex.from_documents(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed99a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your index to a index.json file\n",
    "index.save_to_disk('index.json')\n",
    "# Load the index from your saved index.json file\n",
    "index = GPTSimpleVectorIndex.load_from_disk('index.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18731b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1448 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 11 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I think Facebook's LLaMa is a great step forward in democratizing access to large language models and advancing research in this subfield of AI. It is encouraging to see that they are making the model available at several sizes and providing a model card to detail how it was built in accordance with responsible AI practices. I am also glad to see that they are releasing the model under a noncommercial license to ensure integrity and prevent misuse.\n"
     ]
    }
   ],
   "source": [
    "# Query your index!\n",
    "\n",
    "response = index.query(\"What do you think of Facebook's LLaMa?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57864389",
   "metadata": {},
   "source": [
    "# Customize your LLM for different output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c21c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c726dc51",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'llm_predictor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m max_chunk_overlap \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[0;32m     16\u001b[0m prompt_helper \u001b[39m=\u001b[39m PromptHelper(max_input_size, num_output, max_chunk_overlap)\n\u001b[1;32m---> 18\u001b[0m custom_LLM_index \u001b[39m=\u001b[39m GPTSimpleVectorIndex\u001b[39m.\u001b[39;49mfrom_documents(\n\u001b[0;32m     19\u001b[0m     documents, \n\u001b[0;32m     20\u001b[0m     llm_predictor\u001b[39m=\u001b[39;49mllm_predictor,\n\u001b[0;32m     21\u001b[0m      \u001b[39m# prompt_helper=prompt_helper\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\HYU\\anaconda3\\envs\\openapi\\lib\\site-packages\\llama_index\\indices\\base.py:100\u001b[0m, in \u001b[0;36mBaseGPTIndex.from_documents\u001b[1;34m(cls, documents, docstore, service_context, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m     docstore\u001b[39m.\u001b[39mset_document_hash(doc\u001b[39m.\u001b[39mget_doc_id(), doc\u001b[39m.\u001b[39mget_doc_hash())\n\u001b[0;32m     98\u001b[0m nodes \u001b[39m=\u001b[39m service_context\u001b[39m.\u001b[39mnode_parser\u001b[39m.\u001b[39mget_nodes_from_documents(documents)\n\u001b[1;32m--> 100\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\n\u001b[0;32m    101\u001b[0m     nodes\u001b[39m=\u001b[39mnodes,\n\u001b[0;32m    102\u001b[0m     docstore\u001b[39m=\u001b[39mdocstore,\n\u001b[0;32m    103\u001b[0m     service_context\u001b[39m=\u001b[39mservice_context,\n\u001b[0;32m    104\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    105\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\HYU\\anaconda3\\envs\\openapi\\lib\\site-packages\\llama_index\\indices\\vector_store\\vector_indices.py:69\u001b[0m, in \u001b[0;36mGPTSimpleVectorIndex.__init__\u001b[1;34m(self, nodes, index_struct, service_context, vector_store, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     61\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     62\u001b[0m     nodes: Optional[Sequence[Node]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m     67\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Init params.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m     70\u001b[0m         nodes\u001b[39m=\u001b[39mnodes,\n\u001b[0;32m     71\u001b[0m         index_struct\u001b[39m=\u001b[39mindex_struct,\n\u001b[0;32m     72\u001b[0m         service_context\u001b[39m=\u001b[39mservice_context,\n\u001b[0;32m     73\u001b[0m         vector_store\u001b[39m=\u001b[39mvector_store,\n\u001b[0;32m     74\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m     75\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\HYU\\anaconda3\\envs\\openapi\\lib\\site-packages\\llama_index\\indices\\vector_store\\base.py:54\u001b[0m, in \u001b[0;36mGPTVectorStoreIndex.__init__\u001b[1;34m(self, nodes, index_struct, service_context, vector_store, use_async, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vector_store \u001b[39m=\u001b[39m vector_store \u001b[39mor\u001b[39;00m SimpleVectorStore()\n\u001b[0;32m     53\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_async \u001b[39m=\u001b[39m use_async\n\u001b[1;32m---> 54\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m     55\u001b[0m     nodes\u001b[39m=\u001b[39mnodes,\n\u001b[0;32m     56\u001b[0m     index_struct\u001b[39m=\u001b[39mindex_struct,\n\u001b[0;32m     57\u001b[0m     service_context\u001b[39m=\u001b[39mservice_context,\n\u001b[0;32m     58\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m     59\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'llm_predictor'"
     ]
    }
   ],
   "source": [
    "# Setup your LLM\n",
    "\n",
    "from llama_index import LLMPredictor, GPTSimpleVectorIndex, PromptHelper\n",
    "\n",
    "\n",
    "# define LLM\n",
    "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.1, model_name=\"Ada\"))\n",
    "\n",
    "# define prompt helper\n",
    "# set maximum input size\n",
    "max_input_size = 4096\n",
    "# set number of output tokens\n",
    "num_output = 256\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "\n",
    "custom_LLM_index = GPTSimpleVectorIndex.from_documents(\n",
    "    documents, \n",
    "    llm_predictor=llm_predictor,\n",
    "     # prompt_helper=prompt_helper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f359b0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1404 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The challenges of LLaMa include bias, toxicity, and the potential for generating misinformation. Additionally, there is still more research that needs to be done to address the risks of bias, toxic comments, and hallucinations in large language models.\n"
     ]
    }
   ],
   "source": [
    "# Query your index!\n",
    "\n",
    "response = custom_LLM_index.query(\"challenges of LLaMa?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458f4e5c",
   "metadata": {},
   "source": [
    "# Wikipedia Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4db9772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "loader = WikipediaReader()\n",
    "wikidocs = loader.load_data(pages=['Cyclone Freddy'])\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Cyclone_Freddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "941c9bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 4103 tokens\n"
     ]
    }
   ],
   "source": [
    "wiki_index = GPTSimpleVectorIndex(wikidocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e4a4dd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3844 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cyclone Freddy is a very intense tropical cyclone that affected the Mascarene Islands, Madagascar, Mozambique, and Zimbabwe in February 2023. It is the longest-lived tropical cyclone on record, surpassing Hurricane John's record of 31 days. Freddy was once a powerful cyclone that was classified as a Category 5-equivalent tropical cyclone by the Joint Typhoon Warning Center (JTWC). It caused widespread damage and at least 29 deaths in Madagascar, Mozambique, and Zimbabwe. In Madagascar, over 14,000 homes were affected, with 5,500 destroyed, 3,079 flooded, and at least 9,696 damaged. At least 24,358 people were displaced, and nearly 25,000 customers were left without power at the height of the cyclone. In Saint-Paul, 20 tons of mangoes were destroyed, and Highway RD48 in Salazie was closed due to a landslide. Eleven mobile sites maintained by Orange S.A. were knocked offline in Tampon, Saint-Louis, and Saint-Paul.\n"
     ]
    }
   ],
   "source": [
    "response = wiki_index.query(\"What is cyclone freddy?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebea2acc",
   "metadata": {},
   "source": [
    "# Customer Support Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19f396a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('./asos').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06bc9cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "custom_LLM_index = GPTSimpleVectorIndex.from_documents(\n",
    "    documents, \n",
    "    #llm_predictor=llm_predictor,\n",
    "     # prompt_helper=prompt_helper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cb30944e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 12584 tokens\n"
     ]
    }
   ],
   "source": [
    "index = GPTSimpleVectorIndex(documents)\n",
    "response = index.query(\"What premier service options do I have in the UAE?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "946c44ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1317 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 11 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In the United Arab Emirates, you have the option of signing up for ASOS Premier, which gives you free Standard and Express delivery all year round when you spend over 150 AED. It costs 200 AED and is valid on the order you purchase it on.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"What premier service options do I have in the UAE?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e77e646",
   "metadata": {},
   "source": [
    "# YouTube Video Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "89160742",
   "metadata": {},
   "outputs": [],
   "source": [
    "YoutubeTranscriptReader = download_loader(\"YoutubeTranscriptReader\")\n",
    "\n",
    "loader = YoutubeTranscriptReader()\n",
    "documents = loader.load_data(ytlinks=['https://www.youtube.com/watch?v=K7Kh9Ntd8VE&ab_channel=DaveNick'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0995d23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 18181 tokens\n"
     ]
    }
   ],
   "source": [
    "index = GPTSimpleVectorIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "194e88fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 4024 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Re-uploading other people's content without permission.\n",
      "2. Using copyrighted music.\n",
      "3. Not understanding how the YouTube algorithm works.\n",
      "4. Not researching the best niche for YouTube automation.\n",
      "5. Not optimizing the About section with relevant keywords.\n",
      "6. Not creating a logo and channel art that is professional and attractive.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"What some YouTube automation mistakes to avoid?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359a05da",
   "metadata": {},
   "source": [
    "# Chatbot Class - Just include your index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65a0e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, api_key, index):\n",
    "        self.index = index\n",
    "        openai.api_key = api_key\n",
    "        self.chat_history = []\n",
    "\n",
    "    def generate_response(self, user_input):\n",
    "        prompt = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in self.chat_history[-5:]])\n",
    "        prompt += f\"\\nUser: {user_input}\"\n",
    "        response = index.query(user_input)\n",
    "\n",
    "        message = {\"role\": \"assistant\", \"content\": response.response}\n",
    "        self.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.chat_history.append(message)\n",
    "        return message\n",
    "    \n",
    "    def load_chat_history(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                self.chat_history = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    def save_chat_history(self, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.chat_history, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d3da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('./data').load_data()\n",
    "index = GPTSimpleVectorIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24576df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:openai:error_code=None error_message=\"[''] is not valid under any of the given schemas - 'input'\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "INFO:openai:error_code=None error_message=\"[''] is not valid under any of the given schemas - 'input'\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "INFO:openai:error_code=None error_message=\"[''] is not valid under any of the given schemas - 'input'\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "INFO:openai:error_code=None error_message=\"[''] is not valid under any of the given schemas - 'input'\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "INFO:openai:error_code=None error_message=\"[''] is not valid under any of the given schemas - 'input'\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "INFO:openai:error_code=None error_message=\"[''] is not valid under any of the given schemas - 'input'\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Swap out your index below for whatever knowledge base you want\n",
    "bot = Chatbot(openai_key, index=index)\n",
    "bot.load_chat_history(\"chat_history.json\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"bye\", \"goodbye\"]:\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        bot.save_chat_history(\"chat_history.json\")\n",
    "        break\n",
    "    response = bot.generate_response(user_input)\n",
    "    print(f\"Bot: {response['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029e7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Dataframe Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e61defcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1088cfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HYU\\AppData\\Local\\Temp\\ipykernel_20148\\4065407287.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.any and Series.any will be keyword-only.\n",
      "  data=data2[~data2.isin([np.nan, np.inf, -np.inf]).any(1)]\n"
     ]
    }
   ],
   "source": [
    "data2=pd.read_csv('https://raw.githubusercontent.com/XUAN-24601/Korea_Spatial/main/Data/od_index_transit_sample.csv')#.sample(1000)\n",
    "\n",
    "data=data2[~data2.isin([np.nan, np.inf, -np.inf]).any(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6efc2109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'ADM_CD_O', 'ADM_CD_D', 'flux_2020', 'flux_2021',\n",
       "       'flux_2022', 'flux_2023', 'flux_%20-21', 'flux_%20-22', 'flux_%20-23',\n",
       "       'Resilience_%20-21', 'Resilience_%20-22', 'Resilience_%20-23',\n",
       "       'Mean_time', 'Line_OD', 'Distance', 'mode', 'Densi_bus_count_O',\n",
       "       'Densi_train_O', 'I-Consu&Serv_O', 'I-PubAdmin_O',\n",
       "       'I-Health&SocialWork_O', 'I-Manuf&Stor_O', 'I-Fina&Tech_O',\n",
       "       'University_O', 'Local leisure_O', 'Densi_Tourism_O', 'poi_count_O',\n",
       "       'Entropy_O', 'Car_own_O', 'ReDensity_O', '%foreigner_O',\n",
       "       '%female_employee_O', 'Densi_bus_count_D', 'Densi_train_D',\n",
       "       'I-Consu&Serv_D', 'I-PubAdmin_D', 'I-Health&SocialWork_D',\n",
       "       'I-Manuf&Stor_D', 'I-Fina&Tech_D', 'University_D', 'Local leisure_D',\n",
       "       'Densi_Tourism_D', 'poi_count_D', 'Entropy_D', 'Car_own_D',\n",
       "       'ReDensity_D', '%foreigner_D', '%female_employee_D'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a824bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to count the rows\n",
      "Action: python_repl_ast\n",
      "Action Input: len(df)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m21906\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: There are 21906 rows in the dataframe.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are 21906 rows in the dataframe.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_pandas_dataframe_agent\n",
    "from langchain.llms import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "869a2aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find the maximum value in the flux_2023 column\n",
      "Action: python_repl_ast\n",
      "Action Input: df['flux_2023'].max()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m8210.4\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 8210.4\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'8210.4'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = create_pandas_dataframe_agent(OpenAI(temperature=0), data, verbose=True)\n",
    "agent.run(\"what's the max ridership in 2023?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b498f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find the correlation between two columns\n",
      "Action: python_repl_ast\n",
      "Action Input: df[['flux_2023', 'Car_own_O']].corr()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m           flux_2023  Car_own_O\n",
      "flux_2023   1.000000  -0.027715\n",
      "Car_own_O  -0.027715   1.000000\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The correlation between ridership in 2023 and Car ownership at Origins is -0.027715.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The correlation between ridership in 2023 and Car ownership at Origins is -0.027715.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = create_pandas_dataframe_agent(OpenAI(temperature=0), data, verbose=True)\n",
    "agent.run(\"find the corelation between the ridership in 2023 and Car ownership at Origins (O) \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
